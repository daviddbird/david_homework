---
title: "Homework"
output: html_notebook
---

Load the diamonds.csv data set and undertake an initial exploration of the data. You will find a description of the meanings of the variables on the relevant Kaggle page

```{r}
library(readr)
diamonds <- read_csv("data/diamonds.csv")
```

```{r}
library(GGally)
ggpairs(diamonds)
```
```{r}
library(tidyverse)
glimpse(diamonds)
```

```{r}
summary(diamonds)
```

```{r}
library(psych)
pairs.panels(diamonds[c("X1", "carat", "cut", "color", "clarity", "depth", "table", "price")])
```

```{r}
pairs.panels(diamonds[c("X1", "carat", "x", "y", "z", "price")])
```

We expect the carat of the diamonds to be strong correlated with the physical dimensions x, y and z. Use ggpairs() to investigate correlations between these four variables.
```{r}
pairs.panels(diamonds[c("carat", "x", "y", "z")])
```

So, we do find significant correlations. Let’s drop columns x, y and z from the dataset, in preparation to use only carat going forward.

```{r}
diamonds_trim <- diamonds %>%
  select(-c("x", "y", "z"))
```

```{r}
diamonds_trim
```

We are interested in developing a regression model for the price of a diamond in terms of the possible predictor variables in the dataset.

Use ggpairs() to investigate correlations between price and the predictors (this may take a while to run, don’t worry, make coffee or something).

Perform further ggplot visualisations of any significant correlations you find.

```{r}
ggpairs(diamonds_trim)
```

```{r}
pairs.panels(diamonds[c("carat", "cut", "clarity", "color", "price")])
```

```{r}
diamonds_trim %>%
  ggplot(aes(x = clarity, y = price)) +
  geom_boxplot()
```

```{r}
diamonds_trim %>%
  ggplot(aes(x = cut, y = price)) +
  geom_boxplot()
```

```{r}
diamonds_trim %>%
  ggplot(aes(x = color, y = price)) +
  geom_boxplot()
```
Shortly we may try a regression fit using one or more of the categorical predictors cut, clarity and color, so let’s investigate these predictors:

Investigate the factor levels of these predictors. How many dummy variables do you expect for each of them?

Use the dummy_cols() function in the fastDummies package to generate dummies for these predictors and check the number of dummies in each case.

clarity = 8
cut = 5
color = 7

```{r}
library(fastDummies)
```

```{r}
diamonds_trim <- dummy_cols(diamonds_trim, 
                            select_columns = c("clarity"), 
                            remove_first_dummy = TRUE)

```


```{r}
diamonds_trim <- dummy_cols(diamonds_trim, 
                            select_columns = c("cut"), 
                            remove_first_dummy = TRUE)
```


```{r}
diamonds_trim <- dummy_cols(diamonds_trim, 
                            select_columns = c("color"), 
                            remove_first_dummy = TRUE)
```

```{r}
diamonds_trim
```
Going forward we’ll let R handle dummy variable creation for categorical predictors in regression fitting (remember lm() will generate the correct numbers of dummy levels automatically, absorbing one of the levels into the intercept as a reference level)

First, we’ll start with simple linear regression. Regress price on carat and check the regression diagnostics.

Run a regression with one or both of the predictor and response variables log() transformed and recheck the diagnostics. Do you see any improvement?

Let’s use log() transformations of both predictor and response. Next, experiment with adding a single categorical predictor into the model. Which categorical predictor is best? [Hint - investigate r2
 values]

Interpret the fitted coefficients for the levels of your chosen categorical predictor. Which level is the reference level? Which level shows the greatest difference in price from the reference level? [Hints - remember we are regressing the log(price) here, and think about what the presence of the log(carat) predictor implies. We’re not expecting a mathematical explanation]

```{r}
lin_model <- lm(price ~ carat, data = diamonds_trim)

plot(lin_model)
```

```{r}
summary(lin_model)
```

```{r}
lin_model_log_price <- lm(log(price) ~ carat, data = diamonds_trim)

plot(lin_model_log_price)
summary(lin_model_log_price)
```

```{r}
lin_model_log_price_carat <- lm(log(price) ~ log(carat), data = diamonds_trim)

plot(lin_model_log_price_carat)
summary(lin_model_log_price_carat)
```

```{r}
lin_model_log_price_carat_clarity <- lm(log(price) ~ log(carat) + clarity, 
                                        data = diamonds_trim)

plot(lin_model_log_price_carat_clarity)
summary(lin_model_log_price_carat_clarity)
```

```{r}
lin_model_log_price_carat_cut <- lm(log(price) ~ log(carat) + cut, 
                                        data = diamonds_trim)

plot(lin_model_log_price_carat_cut)
summary(lin_model_log_price_carat_cut)
```

```{r}
lin_model_log_price_carat_color <- lm(log(price) ~ log(carat) + color, 
                                        data = diamonds_trim)

plot(lin_model_log_price_carat_color)
summary(lin_model_log_price_carat_color)
```

Let’s use log() transformations of both predictor and response. Next, experiment with adding a single categorical predictor into the model. Which categorical predictor is best? [Hint - investigate r2
 values]

**Clarity**

Interpret the fitted coefficients for the levels of your chosen categorical predictor. Which level is the reference level? 
I1 is the reference level.

Which level shows the greatest difference in price from the reference level? [Hints - remember we are regressing the log(price) here, and think about what the presence of the log(carat) predictor implies. We’re not expecting a mathematical explanation]

**clarityIF   1.114625** 
Shows the greatest difference in price from the reference level.

log(carat) predictor -


Extension:
Could not get this to run.
```{r}
library(ggiraphExtra)
model_interaction <- lm(log(price) ~ log(carat) + clarity + log(carat):clarity, data = diamonds_trim)

ggPredict(model_interaction)
summary(model_interaction)
```

